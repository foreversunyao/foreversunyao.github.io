---
layout: post
title: "K8s tune"
date: 2021-12-04 19:25:06
description: Kubernetes tune
tags:
 - k8s
---
## Nodes and kernel
1. gke
```
1-5 nodes: n1-standard-1
6-10 nodes: n1-standard-2
11-100 nodes: n1-standard-4
101-250 nodes: n1-standard-8
251-500 nodes: n1-standard-16
more than 500 nodes: n1-standard-32
```
2. kernel
```
# max-file 表示系统级别的能够打开的文件句柄的数量， 一般如果遇到文件句柄达到上限时，会碰到"Too many open files"或者Socket/File: Can’t open so many files等错误。
fs.file-max=1000000

# 配置arp cache 大小
net.ipv4.neigh.default.gc_thresh1=1024
# 存在于ARP高速缓存中的最少层数，如果少于这个数，垃圾收集器将不会运行。缺省值是128。

# 保存在 ARP 高速缓存中的最多的记录软限制。垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。缺省值是 512。
net.ipv4.neigh.default.gc_thresh2=4096

# 保存在 ARP 高速缓存中的最多记录的硬限制，一旦高速缓存中的数目高于此，垃圾收集器将马上运行。缺省值是1024。
net.ipv4.neigh.default.gc_thresh3=8192

# 以上三个参数，当内核维护的arp表过于庞大时候，可以考虑优化


# 允许的最大跟踪连接条目，是在内核内存中netfilter可以同时处理的“任务”（连接跟踪条目）
net.netfilter.nf_conntrack_max=10485760

# 哈希表大小（只读）（64位系统、8G内存默认 65536，16G翻倍，如此类推）
net.core.netdev_max_backlog=10000
# 每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。
net.netfilter.nf_conntrack_tcp_timeout_established=300
net.netfilter.nf_conntrack_buckets=655360

# 关于conntrack的详细说明：https://testerhome.com/topics/7509

# 默认值: 128 指定了每一个real user ID可创建的inotify instatnces的数量上限
fs.inotify.max_user_instances=524288


# 默认值: 8192 指定了每个inotify instance相关联的watches的上限
fs.inotify.max_user_watches=524288
```

## Etcd
1. HA
2. ssd
3. --quota-backend-bytes to increase storage
4. separate etcd cluster

## image
1. docker
```
max-concurrent-downloads=10
concurrent pull 5
ssd
preload pause image
```
2. kubelet
```
--serialize-image-pulls=false
--image-pull-progress-deadline=30
--max-pods=110
```
3. registry p2p

## APIServer
1. nodes 1k-3k
```
--max-requests-inflight=1500
--max-mutating-requests-inflight=500
```
2. mem
```
--target-ram-mb=node_nums * 60
```

## Pod
1. requests&limits
```
spec.containers[].resources.limits.cpu
spec.containers[].resources.limits.memory
spec.containers[].resources.requests.cpu
spec.containers[].resources.requests.memory
spec.containers[].resources.limits.ephemeral-storage
spec.containers[].resources.requests.ephemeral-storage
```
2. nodeAffinity, podAffinity, podAntiAffinity

## Kube-scheduler
1. --kube-api-qps=100

## Kube-controller-manager
1. --kube-api-qps=100 and --kube-api-burst=100

